{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "B_final_report.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VudvGNqcRn3r",
        "mdNggiMEUV2v"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNTgFsP0KtKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73120bb7-7609-403c-ea00-6c6c111c11a5"
      },
      "source": [
        "from google.colab import files, drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzvAcuMpgd_-"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/content/drive/MyDrive/BUDT758B_FinalProject')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11wd5Jo1j-ZI"
      },
      "source": [
        "### Reshaping Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "UCcz3MWhg-7X",
        "outputId": "0fc998bf-d2da-4f0e-8c0e-6fe134b7b616"
      },
      "source": [
        "# Read the data into a pandas data frame:\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import gzip\r\n",
        "\r\n",
        "def parse(path):\r\n",
        "  g = gzip.open(path, 'rb')\r\n",
        "  for l in g:\r\n",
        "    yield json.loads(l)\r\n",
        "\r\n",
        "def getDF(path):\r\n",
        "  i = 0\r\n",
        "  df = {}\r\n",
        "  for d in parse(path):\r\n",
        "    df[i] = d\r\n",
        "    i += 1\r\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\r\n",
        "\r\n",
        "beauty = getDF('All_Beauty.json.gz')\r\n",
        "beauty.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>vote</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>09 1, 2016</td>\n",
              "      <td>A3CIUOJXQ5VDQ2</td>\n",
              "      <td>B0000530HU</td>\n",
              "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
              "      <td>Shelly F</td>\n",
              "      <td>As advertised. Reasonably priced</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>1472688000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>11 14, 2013</td>\n",
              "      <td>A3H7T87S984REU</td>\n",
              "      <td>B0000530HU</td>\n",
              "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
              "      <td>houserules18</td>\n",
              "      <td>Like the oder and the feel when I put it on my...</td>\n",
              "      <td>Good for the face</td>\n",
              "      <td>1384387200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "      <td>08 18, 2013</td>\n",
              "      <td>A3J034YH7UG4KT</td>\n",
              "      <td>B0000530HU</td>\n",
              "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
              "      <td>Adam</td>\n",
              "      <td>I bought this to smell nice after I shave.  Wh...</td>\n",
              "      <td>Smells awful</td>\n",
              "      <td>1376784000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>05 3, 2011</td>\n",
              "      <td>A2UEO5XR3598GI</td>\n",
              "      <td>B0000530HU</td>\n",
              "      <td>{'Size:': ' 7.0 oz', 'Flavor:': ' Classic Ice ...</td>\n",
              "      <td>Rich K</td>\n",
              "      <td>HEY!! I am an Aqua Velva Man and absolutely lo...</td>\n",
              "      <td>Truth is There IS Nothing Like an AQUA VELVA MAN.</td>\n",
              "      <td>1304380800</td>\n",
              "      <td>25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>05 6, 2011</td>\n",
              "      <td>A3SFRT223XXWF7</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>{'Size:': ' 200ml/6.7oz'}</td>\n",
              "      <td>C. C. Christian</td>\n",
              "      <td>If you ever want to feel pampered by a shampoo...</td>\n",
              "      <td>Bvlgari Shampoo</td>\n",
              "      <td>1304640000</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>05 16, 2010</td>\n",
              "      <td>A24HQ2N7332W7W</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>{'Size:': ' 366'}</td>\n",
              "      <td>Kindle Customer Joyce Wilson</td>\n",
              "      <td>If you know the scent of Diva, you'll LOVE thi...</td>\n",
              "      <td>Diva is Heavenly</td>\n",
              "      <td>1273968000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5.0</td>\n",
              "      <td>False</td>\n",
              "      <td>05 7, 2018</td>\n",
              "      <td>A2G90R2ZU6KU5D</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>{'Size:': ' Small'}</td>\n",
              "      <td>Mike</td>\n",
              "      <td>Got this shampoo as a solution for my wife's d...</td>\n",
              "      <td>Outstanding, no complains</td>\n",
              "      <td>1525651200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.0</td>\n",
              "      <td>True</td>\n",
              "      <td>05 7, 2018</td>\n",
              "      <td>A24W4W9E62FZP2</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>{'Size:': ' Small'}</td>\n",
              "      <td>Reb</td>\n",
              "      <td>No change my scalp still itches like crazy. It...</td>\n",
              "      <td>No change my scalp still itches like crazy. It...</td>\n",
              "      <td>1525651200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "      <td>05 6, 2018</td>\n",
              "      <td>A7ID5H7FWLJHC</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>{'Size:': ' Small'}</td>\n",
              "      <td>U. V.</td>\n",
              "      <td>Too expensive for such poor quality. There was...</td>\n",
              "      <td>Too expensive for such poor quality. There was...</td>\n",
              "      <td>1525564800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "      <td>05 6, 2018</td>\n",
              "      <td>AYKOSAJTP5AVS</td>\n",
              "      <td>B00006L9LC</td>\n",
              "      <td>{'Size:': ' Small'}</td>\n",
              "      <td>Senthil Kumar M</td>\n",
              "      <td>It dries my hair, doesnt help to reduce dandru...</td>\n",
              "      <td>Dries my hair, doesnt help to reduce dandruff....</td>\n",
              "      <td>1525564800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall  verified   reviewTime  ... unixReviewTime vote image\n",
              "0      5.0      True   09 1, 2016  ...     1472688000  NaN   NaN\n",
              "1      5.0      True  11 14, 2013  ...     1384387200  NaN   NaN\n",
              "2      1.0      True  08 18, 2013  ...     1376784000  NaN   NaN\n",
              "3      5.0     False   05 3, 2011  ...     1304380800   25   NaN\n",
              "4      5.0      True   05 6, 2011  ...     1304640000    3   NaN\n",
              "5      5.0     False  05 16, 2010  ...     1273968000  NaN   NaN\n",
              "6      5.0     False   05 7, 2018  ...     1525651200  NaN   NaN\n",
              "7      2.0      True   05 7, 2018  ...     1525651200  NaN   NaN\n",
              "8      1.0      True   05 6, 2018  ...     1525564800  NaN   NaN\n",
              "9      1.0      True   05 6, 2018  ...     1525564800  NaN   NaN\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1XFR3cnLg_vs",
        "outputId": "630e9c38-8753-4574-9a62-4a6c9dc74daa"
      },
      "source": [
        "import numpy as np\r\n",
        "beauty.overall.unique()\r\n",
        "beauty['attitude'] = np.where((beauty.overall > 3), 'pos', 'neg')\r\n",
        "beauty_filtered = beauty[['reviewText','attitude']]\r\n",
        "beauty_filtered"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewText</th>\n",
              "      <th>attitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As advertised. Reasonably priced</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Like the oder and the feel when I put it on my...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I bought this to smell nice after I shave.  Wh...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HEY!! I am an Aqua Velva Man and absolutely lo...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you ever want to feel pampered by a shampoo...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5264</th>\n",
              "      <td>I have genetic undereye darkness. Ive accepted...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5265</th>\n",
              "      <td>I absolutely love this eye gel.</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5266</th>\n",
              "      <td>The eye gel is easy to apply and I use it morn...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5267</th>\n",
              "      <td>Ok this eye gel is good stuff.</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5268</th>\n",
              "      <td>This is the first eye gel/cream that actually ...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5269 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             reviewText attitude\n",
              "0                      As advertised. Reasonably priced      pos\n",
              "1     Like the oder and the feel when I put it on my...      pos\n",
              "2     I bought this to smell nice after I shave.  Wh...      neg\n",
              "3     HEY!! I am an Aqua Velva Man and absolutely lo...      pos\n",
              "4     If you ever want to feel pampered by a shampoo...      pos\n",
              "...                                                 ...      ...\n",
              "5264  I have genetic undereye darkness. Ive accepted...      pos\n",
              "5265                    I absolutely love this eye gel.      pos\n",
              "5266  The eye gel is easy to apply and I use it morn...      pos\n",
              "5267                     Ok this eye gel is good stuff.      pos\n",
              "5268  This is the first eye gel/cream that actually ...      pos\n",
              "\n",
              "[5269 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lVFoOvrhDSt"
      },
      "source": [
        "beauty_pos = beauty_filtered[beauty_filtered['attitude']=='pos']\r\n",
        "beauty_neg = beauty_filtered[beauty_filtered['attitude']=='neg']\r\n",
        "\r\n",
        "print(\"pos shape:\" + str(beauty_pos.shape))\r\n",
        "print(\"neg shape:\" + str(beauty_neg.shape))\r\n",
        "\r\n",
        "for i in range(4981):\r\n",
        "  save_path = '/content/drive/My Drive/BUDT758B_FinalProject/pos/'\r\n",
        "  value = str(beauty_pos.iloc[i]['reviewText'])\r\n",
        "  filename = os.path.join(save_path, 'pos' + str(10000+i) +'.txt')\r\n",
        "  f = open(filename,\"w\")\r\n",
        "  f.write(value)\r\n",
        "  f.close\r\n",
        "\r\n",
        "for i in range(288):\r\n",
        "  save_path = '/content/drive/My Drive/BUDT758B_FinalProject/neg/'\r\n",
        "  value = str(beauty_neg.iloc[i]['reviewText'])\r\n",
        "  filename = os.path.join(save_path, 'neg' + str(10000+i) +'.txt')\r\n",
        "  f = open(filename,\"w\")\r\n",
        "  f.write(value)\r\n",
        "  f.close\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCtNCjpMX-gV"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qedTuR8kYD6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d468793-fa6e-4a9f-d4e1-8865372a15a0"
      },
      "source": [
        "import os, sys\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import collections\n",
        "import statistics\n",
        "\n",
        "pos_root_dir = '/content/drive/My Drive/BUDT758B_FinalProject/pos/'\n",
        "neg_root_dir = '/content/drive/My Drive/BUDT758B_FinalProject/neg/'\n",
        "pos_files = os.listdir(pos_root_dir)\n",
        "neg_files = os.listdir(neg_root_dir)\n",
        "\n",
        "corpus = []\n",
        "labels = []\n",
        "for i in range(2500):\n",
        "  f = pos_files[i]\n",
        "  with open(pos_root_dir+f) as fh:\n",
        "    corpus.append(fh.read().replace('\\n',' '))\n",
        "    labels.append([1,0])\n",
        "for i in range(288):\n",
        "  f = neg_files[i]\n",
        "  with open(neg_root_dir+f) as fh:\n",
        "    corpus.append(fh.read().replace('\\n',' '))\n",
        "    labels.append([0,1])\n",
        "\n",
        "\n",
        "# Remove the punctuation\n",
        "corpus = [''.join(c for c in s if c not in punctuation) for s in corpus]\n",
        "\n",
        "# Convert all the letters to lower case.\n",
        "for i in range(len(corpus)):\n",
        "  corpus[i] = corpus[i].lower()\n",
        "\n",
        "# outlier review stats & Determine the max features\n",
        "review_lens = collections.Counter([len(x) for x in corpus])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
        "print(\"Minimum review length: {}\".format(min(review_lens)))\n",
        "print(\"Median review length: {}\".format(statistics.median(review_lens)))\n",
        "\n",
        "max_features = 200\n",
        "\n",
        "# Remove the stop words in English\n",
        "vectorizer = TfidfVectorizer(max_features = max_features, stop_words = 'english')\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "y = np.array(labels)\n",
        "print(X.shape,y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 4\n",
            "Maximum review length: 3520\n",
            "Minimum review length: 0\n",
            "Median review length: 317.5\n",
            "(2788, 200) (2788, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOhmTts3Lo8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5977b39c-eace-474f-d93b-8bdea12d871e"
      },
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seq_length = -1\n",
        "\n",
        "word_tokenizer = vectorizer.build_tokenizer()\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "doc_terms_list_train = [word_tokenizer(doc_str) for doc_str in corpus]\n",
        "docs = []\n",
        "for i in range(len(doc_terms_list_train)):\n",
        "  terms = []\n",
        "  for j in range(len(doc_terms_list_train[i])):\n",
        "    w = doc_terms_list_train[i][j]\n",
        "    if w in vocab:\n",
        "      terms.append(w)\n",
        "  if len(terms) > seq_length:\n",
        "    seq_length = len(terms)\n",
        "  docs.append(terms)\n",
        "\n",
        "datasets = np.zeros((X.shape[0],seq_length,max_features))\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  # Padding\n",
        "  n_padding = seq_length - len(docs[i])\n",
        "\n",
        "  for j in range(len(docs[i])):\n",
        "    w = docs[i][j]\n",
        "    idx = vocab[w]\n",
        "    tfidf_val = X[i,idx]\n",
        "    datasets[i,j+n_padding,idx] = tfidf_val\n",
        "\n",
        "datasets = datasets.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "\n",
        "X_train,X_val,y_train,y_val = train_test_split(datasets, y, test_size=0.2, random_state=2020)\n",
        "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2230, 120, 200) (2230, 2) (558, 120, 200) (558, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VudvGNqcRn3r"
      },
      "source": [
        "\n",
        "### DataLoaders and Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXrDBe8qRzte"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train))\n",
        "val_data = TensorDataset(torch.from_numpy(X_val),torch.from_numpy(y_val))\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_data,shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2VKHjpKLOFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8654a120-5a0a-424a-aa13-d3e9d19c011a"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Model(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "# Define three types of layer.\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,n_layers,batch_first=True)\n",
        "    self.fc1 = nn.Linear(hidden_size,output_size)\n",
        "    self.fc2 = nn.Linear(output_size,2)\n",
        "  \n",
        "  def forward(self,x,hidden):\n",
        "    batch_size = x.size()[0]\n",
        "\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    rnn_out,hidden = self.rnn(x,hidden)\n",
        "    rnn_out = self.fc1(rnn_out)\n",
        "    last_out = rnn_out[:,-1,:].view(batch_size,-1)\n",
        "    out = F.softmax(self.fc2(last_out))\n",
        "\n",
        "    return out,hidden\n",
        "  \n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()\n",
        "    return hidden\n",
        "\n",
        "model = Model(200,32,256,2)   \n",
        "print(model) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (rnn): RNN(200, 256, num_layers=2, batch_first=True)\n",
            "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdNggiMEUV2v"
      },
      "source": [
        "### Trainning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BienkZzhRaBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6e467d-6aba-4648-dbc6-5fc8d5c9f8c2"
      },
      "source": [
        "# train on Gpu\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 10\n",
        "lr = 1e-4\n",
        "counter = 0\n",
        "clip = 5\n",
        "\n",
        "#Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "  # Initialize hidden state\n",
        "  h = model.init_hidden(batch_size)\n",
        "\n",
        "  # Batch loop\n",
        "  for inputs,labels in train_loader:\n",
        "    counter +=1\n",
        "    inputs,labels = inputs.to(device),labels.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    outputs,h = model(inputs,h)\n",
        "\n",
        "    loss = criterion(outputs,torch.max(labels,1)[1])\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm(model.parameters(),clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    #Validation loss\n",
        "    if counter% 10 ==0:\n",
        "      val_h = model.init_hidden(batch_size).cuda()\n",
        "      val_losses = []\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      for inputs,labels in val_loader:\n",
        "        \n",
        "        inputs, labels = inputs.to(device),labels.to(device)\n",
        "        val_outputs, val_h = model(inputs,val_h)\n",
        "        val_loss = criterion(val_outputs,torch.max(labels,1)[1])\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        #pred = torch.round(outputs.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "        # compare predictions to true label\n",
        "        \n",
        "        pred = torch.round(val_outputs.squeeze())\n",
        "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "        accuracy=np.sum(correct)/2/batch_size\n",
        "      model.train()\n",
        "\n",
        "      print('Epoch:{}/{}'.format(epoch+1,n_epochs),\n",
        "            'Batch:{}'.format(counter),\n",
        "            'Train Loss:{:.5f}'.format(loss.item()),\n",
        "            'Val Loss:{:.5f}'.format(np.mean(val_losses)),\n",
        "            'Accuracy:{:.2%}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1/10 Batch:10 Train Loss:0.63751 Val Loss:0.63726 Accuracy:65.00%\n",
            "Epoch:1/10 Batch:20 Train Loss:0.60843 Val Loss:0.52987 Accuracy:90.00%\n",
            "Epoch:1/10 Batch:30 Train Loss:0.34245 Val Loss:0.42995 Accuracy:85.00%\n",
            "Epoch:1/10 Batch:40 Train Loss:0.46558 Val Loss:0.41818 Accuracy:85.00%\n",
            "Epoch:1/10 Batch:50 Train Loss:0.51393 Val Loss:0.41718 Accuracy:75.00%\n",
            "Epoch:1/10 Batch:60 Train Loss:0.36476 Val Loss:0.41636 Accuracy:85.00%\n",
            "Epoch:1/10 Batch:70 Train Loss:0.41419 Val Loss:0.41655 Accuracy:75.00%\n",
            "Epoch:1/10 Batch:80 Train Loss:0.56335 Val Loss:0.41622 Accuracy:80.00%\n",
            "Epoch:1/10 Batch:90 Train Loss:0.36417 Val Loss:0.41613 Accuracy:80.00%\n",
            "Epoch:1/10 Batch:100 Train Loss:0.41388 Val Loss:0.41605 Accuracy:80.00%\n",
            "Epoch:1/10 Batch:110 Train Loss:0.31416 Val Loss:0.41580 Accuracy:85.00%\n",
            "Epoch:2/10 Batch:120 Train Loss:0.51347 Val Loss:0.41634 Accuracy:70.00%\n",
            "Epoch:2/10 Batch:130 Train Loss:0.41373 Val Loss:0.41591 Accuracy:80.00%\n",
            "Epoch:2/10 Batch:140 Train Loss:0.46357 Val Loss:0.41587 Accuracy:80.00%\n",
            "Epoch:2/10 Batch:150 Train Loss:0.36377 Val Loss:0.41563 Accuracy:85.00%\n",
            "Epoch:2/10 Batch:160 Train Loss:0.41362 Val Loss:0.41580 Accuracy:80.00%\n",
            "Epoch:2/10 Batch:170 Train Loss:0.41360 Val Loss:0.41637 Accuracy:65.00%\n",
            "Epoch:2/10 Batch:180 Train Loss:0.41357 Val Loss:0.41595 Accuracy:75.00%\n",
            "Epoch:2/10 Batch:190 Train Loss:0.41355 Val Loss:0.41553 Accuracy:85.00%\n",
            "Epoch:2/10 Batch:200 Train Loss:0.31370 Val Loss:0.41572 Accuracy:80.00%\n",
            "Epoch:2/10 Batch:210 Train Loss:0.56329 Val Loss:0.41530 Accuracy:90.00%\n",
            "Epoch:2/10 Batch:220 Train Loss:0.46343 Val Loss:0.41569 Accuracy:80.00%\n",
            "Epoch:3/10 Batch:230 Train Loss:0.51336 Val Loss:0.41547 Accuracy:85.00%\n",
            "Epoch:3/10 Batch:240 Train Loss:0.41348 Val Loss:0.41526 Accuracy:90.00%\n",
            "Epoch:3/10 Batch:250 Train Loss:0.36353 Val Loss:0.41564 Accuracy:80.00%\n",
            "Epoch:3/10 Batch:260 Train Loss:0.36351 Val Loss:0.41543 Accuracy:85.00%\n",
            "Epoch:3/10 Batch:270 Train Loss:0.36350 Val Loss:0.41542 Accuracy:85.00%\n",
            "Epoch:3/10 Batch:280 Train Loss:0.36349 Val Loss:0.41542 Accuracy:85.00%\n",
            "Epoch:3/10 Batch:290 Train Loss:0.36348 Val Loss:0.41521 Accuracy:90.00%\n",
            "Epoch:3/10 Batch:300 Train Loss:0.46337 Val Loss:0.41560 Accuracy:80.00%\n",
            "Epoch:3/10 Batch:310 Train Loss:0.36345 Val Loss:0.41539 Accuracy:85.00%\n",
            "Epoch:3/10 Batch:320 Train Loss:0.46336 Val Loss:0.41558 Accuracy:80.00%\n",
            "Epoch:3/10 Batch:330 Train Loss:0.31348 Val Loss:0.41538 Accuracy:85.00%\n",
            "Epoch:4/10 Batch:340 Train Loss:0.36343 Val Loss:0.41577 Accuracy:75.00%\n",
            "Epoch:4/10 Batch:350 Train Loss:0.41338 Val Loss:0.41557 Accuracy:80.00%\n",
            "Epoch:4/10 Batch:360 Train Loss:0.46335 Val Loss:0.41596 Accuracy:70.00%\n",
            "Epoch:4/10 Batch:370 Train Loss:0.46334 Val Loss:0.41556 Accuracy:80.00%\n",
            "Epoch:4/10 Batch:380 Train Loss:0.51331 Val Loss:0.41575 Accuracy:75.00%\n",
            "Epoch:4/10 Batch:390 Train Loss:0.41337 Val Loss:0.41555 Accuracy:80.00%\n",
            "Epoch:4/10 Batch:400 Train Loss:0.36339 Val Loss:0.41535 Accuracy:85.00%\n",
            "Epoch:4/10 Batch:410 Train Loss:0.36339 Val Loss:0.41534 Accuracy:85.00%\n",
            "Epoch:4/10 Batch:420 Train Loss:0.51330 Val Loss:0.41553 Accuracy:80.00%\n",
            "Epoch:4/10 Batch:430 Train Loss:0.41335 Val Loss:0.41533 Accuracy:85.00%\n",
            "Epoch:4/10 Batch:440 Train Loss:0.51330 Val Loss:0.41553 Accuracy:80.00%\n",
            "Epoch:5/10 Batch:450 Train Loss:0.46332 Val Loss:0.41533 Accuracy:85.00%\n",
            "Epoch:5/10 Batch:460 Train Loss:0.46332 Val Loss:0.41532 Accuracy:85.00%\n",
            "Epoch:5/10 Batch:470 Train Loss:0.46332 Val Loss:0.41532 Accuracy:85.00%\n",
            "Epoch:5/10 Batch:480 Train Loss:0.46331 Val Loss:0.41572 Accuracy:75.00%\n",
            "Epoch:5/10 Batch:490 Train Loss:0.41333 Val Loss:0.41571 Accuracy:75.00%\n",
            "Epoch:5/10 Batch:500 Train Loss:0.36335 Val Loss:0.41512 Accuracy:90.00%\n",
            "Epoch:5/10 Batch:510 Train Loss:0.36335 Val Loss:0.41571 Accuracy:75.00%\n",
            "Epoch:5/10 Batch:520 Train Loss:0.31336 Val Loss:0.41590 Accuracy:70.00%\n",
            "Epoch:5/10 Batch:530 Train Loss:0.46331 Val Loss:0.41511 Accuracy:90.00%\n",
            "Epoch:5/10 Batch:540 Train Loss:0.31336 Val Loss:0.41570 Accuracy:75.00%\n",
            "Epoch:5/10 Batch:550 Train Loss:0.36334 Val Loss:0.41550 Accuracy:80.00%\n",
            "Epoch:5/10 Batch:560 Train Loss:0.51328 Val Loss:0.41550 Accuracy:80.00%\n",
            "Epoch:6/10 Batch:570 Train Loss:0.36333 Val Loss:0.41530 Accuracy:85.00%\n",
            "Epoch:6/10 Batch:580 Train Loss:0.51328 Val Loss:0.41530 Accuracy:85.00%\n",
            "Epoch:6/10 Batch:590 Train Loss:0.36333 Val Loss:0.41510 Accuracy:90.00%\n",
            "Epoch:6/10 Batch:600 Train Loss:0.41331 Val Loss:0.41530 Accuracy:85.00%\n",
            "Epoch:6/10 Batch:610 Train Loss:0.51328 Val Loss:0.41549 Accuracy:80.00%\n",
            "Epoch:6/10 Batch:620 Train Loss:0.41331 Val Loss:0.41549 Accuracy:80.00%\n",
            "Epoch:6/10 Batch:630 Train Loss:0.36332 Val Loss:0.41529 Accuracy:85.00%\n",
            "Epoch:6/10 Batch:640 Train Loss:0.41331 Val Loss:0.41569 Accuracy:75.00%\n",
            "Epoch:6/10 Batch:650 Train Loss:0.46329 Val Loss:0.41549 Accuracy:80.00%\n",
            "Epoch:6/10 Batch:660 Train Loss:0.46329 Val Loss:0.41509 Accuracy:90.00%\n",
            "Epoch:6/10 Batch:670 Train Loss:0.46329 Val Loss:0.41568 Accuracy:75.00%\n",
            "Epoch:7/10 Batch:680 Train Loss:0.51328 Val Loss:0.41548 Accuracy:80.00%\n",
            "Epoch:7/10 Batch:690 Train Loss:0.46329 Val Loss:0.41548 Accuracy:80.00%\n",
            "Epoch:7/10 Batch:700 Train Loss:0.36331 Val Loss:0.41528 Accuracy:85.00%\n",
            "Epoch:7/10 Batch:710 Train Loss:0.36331 Val Loss:0.41568 Accuracy:75.00%\n",
            "Epoch:7/10 Batch:720 Train Loss:0.41330 Val Loss:0.41528 Accuracy:85.00%\n",
            "Epoch:7/10 Batch:730 Train Loss:0.41330 Val Loss:0.41508 Accuracy:90.00%\n",
            "Epoch:7/10 Batch:740 Train Loss:0.41330 Val Loss:0.41508 Accuracy:90.00%\n",
            "Epoch:7/10 Batch:750 Train Loss:0.31332 Val Loss:0.41548 Accuracy:80.00%\n",
            "Epoch:7/10 Batch:760 Train Loss:0.41329 Val Loss:0.41568 Accuracy:75.00%\n",
            "Epoch:7/10 Batch:770 Train Loss:0.31331 Val Loss:0.41548 Accuracy:80.00%\n",
            "Epoch:7/10 Batch:780 Train Loss:0.31331 Val Loss:0.41528 Accuracy:85.00%\n",
            "Epoch:8/10 Batch:790 Train Loss:0.36330 Val Loss:0.41547 Accuracy:80.00%\n",
            "Epoch:8/10 Batch:800 Train Loss:0.46328 Val Loss:0.41508 Accuracy:90.00%\n",
            "Epoch:8/10 Batch:810 Train Loss:0.41329 Val Loss:0.41508 Accuracy:90.00%\n",
            "Epoch:8/10 Batch:820 Train Loss:0.41329 Val Loss:0.41547 Accuracy:80.00%\n",
            "Epoch:8/10 Batch:830 Train Loss:0.51327 Val Loss:0.41527 Accuracy:85.00%\n",
            "Epoch:8/10 Batch:840 Train Loss:0.36330 Val Loss:0.41547 Accuracy:80.00%\n",
            "Epoch:8/10 Batch:850 Train Loss:0.36330 Val Loss:0.41587 Accuracy:70.00%\n",
            "Epoch:8/10 Batch:860 Train Loss:0.36330 Val Loss:0.41567 Accuracy:75.00%\n",
            "Epoch:8/10 Batch:870 Train Loss:0.31330 Val Loss:0.41507 Accuracy:90.00%\n",
            "Epoch:8/10 Batch:880 Train Loss:0.41329 Val Loss:0.41527 Accuracy:85.00%\n",
            "Epoch:8/10 Batch:890 Train Loss:0.41329 Val Loss:0.41527 Accuracy:85.00%\n",
            "Epoch:9/10 Batch:900 Train Loss:0.41329 Val Loss:0.41507 Accuracy:90.00%\n",
            "Epoch:9/10 Batch:910 Train Loss:0.36329 Val Loss:0.41567 Accuracy:75.00%\n",
            "Epoch:9/10 Batch:920 Train Loss:0.36329 Val Loss:0.41507 Accuracy:90.00%\n",
            "Epoch:9/10 Batch:930 Train Loss:0.41328 Val Loss:0.41547 Accuracy:80.00%\n",
            "Epoch:9/10 Batch:940 Train Loss:0.41328 Val Loss:0.41547 Accuracy:80.00%\n",
            "Epoch:9/10 Batch:950 Train Loss:0.41328 Val Loss:0.41507 Accuracy:90.00%\n",
            "Epoch:9/10 Batch:960 Train Loss:0.46328 Val Loss:0.41566 Accuracy:75.00%\n",
            "Epoch:9/10 Batch:970 Train Loss:0.46328 Val Loss:0.41566 Accuracy:75.00%\n",
            "Epoch:9/10 Batch:980 Train Loss:0.51327 Val Loss:0.41546 Accuracy:80.00%\n",
            "Epoch:9/10 Batch:990 Train Loss:0.36329 Val Loss:0.41507 Accuracy:90.00%\n",
            "Epoch:9/10 Batch:1000 Train Loss:0.61326 Val Loss:0.41526 Accuracy:85.00%\n",
            "Epoch:10/10 Batch:1010 Train Loss:0.41328 Val Loss:0.41526 Accuracy:85.00%\n",
            "Epoch:10/10 Batch:1020 Train Loss:0.41328 Val Loss:0.41526 Accuracy:85.00%\n",
            "Epoch:10/10 Batch:1030 Train Loss:0.41328 Val Loss:0.41566 Accuracy:75.00%\n",
            "Epoch:10/10 Batch:1040 Train Loss:0.46327 Val Loss:0.41566 Accuracy:75.00%\n",
            "Epoch:10/10 Batch:1050 Train Loss:0.36328 Val Loss:0.41566 Accuracy:75.00%\n",
            "Epoch:10/10 Batch:1060 Train Loss:0.41328 Val Loss:0.41586 Accuracy:70.00%\n",
            "Epoch:10/10 Batch:1070 Train Loss:0.31329 Val Loss:0.41506 Accuracy:90.00%\n",
            "Epoch:10/10 Batch:1080 Train Loss:0.36328 Val Loss:0.41586 Accuracy:70.00%\n",
            "Epoch:10/10 Batch:1090 Train Loss:0.36328 Val Loss:0.41606 Accuracy:65.00%\n",
            "Epoch:10/10 Batch:1100 Train Loss:0.61326 Val Loss:0.41526 Accuracy:85.00%\n",
            "Epoch:10/10 Batch:1110 Train Loss:0.51327 Val Loss:0.41526 Accuracy:85.00%\n",
            "Epoch:10/10 Batch:1120 Train Loss:0.51327 Val Loss:0.41566 Accuracy:75.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g15hJgFiQkpH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}